{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjDyt5ICi5ec",
        "outputId": "a8a82806-0122-4638-8e38-0fb2f5e05130"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y ctransformers\n",
        "!pip install --no-cache-dir ctransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cDN_ZTPjK-X",
        "outputId": "116a13e0-1ee8-48a1-8b6e-cb942f9c52b7"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/models\n",
        "!ls -l /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD3m6TNUjQWj",
        "outputId": "55c7fb05-17f4-483e-e533-a8a6127d740d"
      },
      "outputs": [],
      "source": [
        "!wget -O /content/models/openhermes-2.5-mistral-7b.Q5_K_M.gguf \"https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q5_K_M.gguf\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ7PyOq4jW5B",
        "outputId": "e241c767-6418-4240-f8da-38baa0fdeabc"
      },
      "outputs": [],
      "source": [
        "!ls -lh /content/models/\n",
        "!file /content/models/openhermes-2.5-mistral-7b.Q5_K_M.gguf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPJKtMPsjZNk",
        "outputId": "3c232580-2edc-4f61-b72d-dd17ecab103f"
      },
      "outputs": [],
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "# Define model path\n",
        "model_path = \"/content/models/openhermes-2.5-mistral-7b.Q5_K_M.gguf\"\n",
        "\n",
        "# Load the model\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    model_type=\"mistral\",\n",
        "    gpu_layers=100,\n",
        "    context_length=4096\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DquL-qaiNZ33",
        "outputId": "bd7e43cb-45ba-42be-cb5e-76e236183a89"
      },
      "outputs": [],
      "source": [
        "print(llm.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWEQ71x6juws",
        "outputId": "235c8e9a-19c2-474a-faf1-0b17f9a9c244"
      },
      "outputs": [],
      "source": [
        "# Generate a response from the model\n",
        "response = llm(\"What are the key skills required for a software developer?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJHmWP0yjyYE",
        "outputId": "99ca3524-fc64-4aa8-96e5-858caab19042"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber chromadb sentence-transformers uvicorn ctransformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr5LRpYaklEc"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import chromadb\n",
        "import uuid\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from ctransformers import AutoModelForCausalLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9rgScBalF81"
      },
      "outputs": [],
      "source": [
        "def generate_response(prompt):\n",
        "    return llm(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DlCY8RGlKX9"
      },
      "outputs": [],
      "source": [
        "# Initialize ChromaDB in-memory\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# Create collection for storing resume embeddings\n",
        "resume_collection = chroma_client.get_or_create_collection(name=\"resumes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOF2fuaNlSFB"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a given PDF file dynamically.\"\"\"\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWNE0oN-lUEf"
      },
      "outputs": [],
      "source": [
        "# Load SentenceTransformer model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize ChromaDB\n",
        "chroma_client = chromadb.PersistentClient(path=\"/content/chroma_db\")\n",
        "resume_collection = chroma_client.get_or_create_collection(\"resume_embeddings\")\n",
        "\n",
        "def chunk_and_store_resume(resume_text):\n",
        "    \"\"\"Splits the extracted resume text into chunks and stores embeddings in ChromaDB.\"\"\"\n",
        "    chunks = resume_text.split(\"\\n\")  # Simple line-based chunking\n",
        "    embeddings = embedding_model.encode(chunks).tolist()\n",
        "\n",
        "    resume_collection.add(\n",
        "        ids=[str(uuid.uuid4()) for _ in chunks],\n",
        "        documents=chunks,\n",
        "        embeddings=embeddings\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkDxs9rSl1-U"
      },
      "outputs": [],
      "source": [
        "def query_resume(job_role):\n",
        "    \"\"\"Queries the resume database for the best matching chunks based on a given job role.\"\"\"\n",
        "    job_embedding = embedding_model.encode([job_role]).tolist()[0]\n",
        "\n",
        "    results = resume_collection.query(\n",
        "        query_embeddings=[job_embedding],\n",
        "        n_results=5\n",
        "    )\n",
        "\n",
        "    return results['documents'][0]  # Return best-matched resume chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI5RbroBlf1O"
      },
      "outputs": [],
      "source": [
        "def analyze_resume(pdf_path, job_role):\n",
        "    \"\"\"Extracts text, processes the resume, and analyzes it using OpenHermes.\"\"\"\n",
        "\n",
        "    # Step 1: Extract Text\n",
        "    resume_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Step 2: Store in Vector Database\n",
        "    chunk_and_store_resume(resume_text)\n",
        "\n",
        "    # Step 3: Query Resume\n",
        "    matched_text = \" \".join(query_resume(job_role))\n",
        "\n",
        "    # Step 4: Generate LLM Prompt\n",
        "    prompt = f\"\"\"\n",
        "    You are an excellent decision maker when it comes to select resume.\n",
        "    Given the following resume details:\n",
        "    {matched_text}\n",
        "\n",
        "    And the job role: {job_role}\n",
        "\n",
        "    Provide:\n",
        "    1. **Observation** – Key strengths from the resume.\n",
        "    2. **Comparison Result** – How well it matches the job role. If the candidate does not have a strong with the {job_role}, describe the gaps and suggest improvement areas to fit the role better.\n",
        "    3. **Conclusion** – If it fits, motivate. If not, suggest missing skills in a list format or alternate career paths to choose with job titles.\n",
        "    If you are not able to answer, respond \"I dont know\", do not make up an answer.\n",
        "    \"\"\"\n",
        "\n",
        "    return generate_response(prompt)  # LLM model generates the response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsfH3XA3lsle",
        "outputId": "ecde8238-eff5-4777-cc8b-4703f5ba0fd1"
      },
      "outputs": [],
      "source": [
        "resume_file = \"/content/Aditya Agarwal CV.pdf\"\n",
        "job_role = \"Frontend Developer with React.js & Node.js experience\"\n",
        "\n",
        "analysis_result = analyze_resume(resume_file, job_role)\n",
        "print(analysis_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_x3K1MHmqNH",
        "outputId": "ad47ef21-5ae5-4b7d-d15d-b2e6155c6eac"
      },
      "outputs": [],
      "source": [
        "resume_file = \"/content/Aditya Agarwal CV.pdf\"\n",
        "job_role = \"QA Test Engineer\"\n",
        "\n",
        "analysis_result = analyze_resume(resume_file, job_role)\n",
        "print(analysis_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7aa9WJ0n4nn",
        "outputId": "93a13053-9e48-4ae0-9635-3897e4d30c05"
      },
      "outputs": [],
      "source": [
        "resume_file = \"/content/Aditya Agarwal CV.pdf\"\n",
        "job_role = \"AI Engineer\"\n",
        "\n",
        "analysis_result = analyze_resume(resume_file, job_role)\n",
        "print(analysis_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZShr4YU7UFHO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
